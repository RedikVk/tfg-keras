\chapter{La librería Keras}
\lsection{Introducción}
Keras es una librería de redes neuronales escrita en Python y capaz de ejecutarse tanto sobre Tensorflow como sobre Theano. Sus principales características son:
\begin{itemize}[noitemsep]
\item Fácil y rápido prototipado gracias a su modularidad, minimalismo y extensibilidad.
\item Soporta tanto redes neuronales convolucionales como recurrentes (así como la combinación de ambas)
\item Soporta esquemas de conectividad arbitrarios (incluyendo entrenamiento multi-entrada y multi-salida)
\item Corre en CPU y GPU
\item Es compatible con Python 2.7-3.5
 \end{itemize}

 \lsection{Instalación}
 Para la instalacion de Keras, son necesarias las siguientes dependencias:
 \begin{itemize}[noitemsep]
\item \lstinline{numpy} y \lstinline{scipy} : librerías cientificas para Python numéricas.
\item \lstinline{pyyaml} : parseador YAML para Python.
\item \lstinline{HDF5} y \lstinline{h5py} (Opcional, pero requírido si se usan funciones para cargar/guardar modelos) : paquete para el manejo de grandes cantidades de datos.
\item \lstinline{cuDNN - Nvidia CUDA} (Opcional pero recomendado si se usan CNNs) : librería para tarjetas gráficas NVIDIA que permite la aceleración de la GPU en temas de redes neuronales.
\item \lstinline{Tensorflow} o \lstinline{Theano} : librerías Python enfocadas a Machine Learning.
 \end{itemize}
Existen 2 modos de instalación:
 \begin{itemize}[noitemsep]
 \item Desde PyPI: \lstinline{sudo pip install keras}
 \item Desde \href{https://github.com/fchollet/keras/tree/master/keras}{repositorio Git}: \lstinline{sudo python setup.py install}
 \end{itemize}
 Por defecto, Keras usa Tensorflow como motor backend. Sin embargo, esta opción puede ser configurada. La primera vez se ejecuta Keras, se crea un fichero de configuración en "/.keras/keras.json", que puede ser modificado.
\begin{lstlisting}
    "image_dim_ordering": "tf",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
\end{lstlisting}

\lsection{Modelos}
Hay dos tipos de modelos disponibles para su implementacion en Keras: el modelo secuencial, y la clase \lstinline{Model} (más general). Ambos modelos presentan varios métodos en común, como \lstinline{summary()}, \lstinline{get_config()} o \lstinline{to_json()} que devuelven información basica sobre el modelo, o \lstinline{get_weights()} y \lstinline{set_weights()}, los métodos \lstinline{getter} y \lstinline{setter} propios de los pesos del modelo.
\subsection{Modelo secuencial}
Los métodos básicas del modelo secuencial son:
\begin{itemize}
\item \lstinline{compile(self, optimizer, loss, metrics=[], sample_weight_mode=None)} : Calcula automáticamente la función gradiente y prepara el modelo para una forma concreta de entrenamiento.
\item \lstinline{fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)} : Entrena el modelo para un número fijado de ciclos (\lstinline{nb_epoch}), con actualizaciones de gradiente cada \lstinline{batch_size} ejemplos.
\item \lstinline{evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)} : Calcula la función de perdida dados unos datos de entrada, lote por lote.
\item \lstinline{predict(self, x, batch_size=32, verbose=0)} : Genera predicciones para unos valores de entrada.
\end{itemize}
\subsection{Clase Model de la API}
Con la clase \lstinline{Model}, dadas una entrada y una salida, podemos inicializar nuestro modelo de la siguiente forma:
\begin{lstlisting}[language=Python]
a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(input=a, output=b)
\end{lstlisting}
En caso de que tengamos múltiples conjuntos de entradas y de salidas, también podríamos inicializarlo de la siguiente manera:
\begin{lstlisting}[language=Python]
model = Model(input=[a1, a2], output=[b1, b3, b3])
\end{lstlisting}
\lsection{Capas}
\subsection{Funciones básicas de las capas}
A la hora de diseñar las capas de nuestra red neuronal, tenemos una serie de funciones disponibles:
\begin{itemize}
\item \lstinline{Dense}: Para capas regulares totalmente conectadas.\\
\lstinline{keras.layers.core.Dense(output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)}
\item \lstinline{Activation}: Se aplica una función de activación a una salida. Las principales funciones que nos ofrece Keras son:
\begin{itemize}
\item \lstinline{softmax}: generalización de la función logística.
\item \lstinline{softplus}
\item \lstinline{softsign}
\item \lstinline{relu}: función rectificadora
\item \lstinline{tanh}: función tangente hiperbólica
\item \lstinline{sigmoid}: función sigmoide
\item \lstinline{hard_sigmoid}
\item \lstinline{linear}: función lineal
\end{itemize}
Podemos encontrar funciones de activación más avanzadas como \lstinline{PReLu} o \lstinline{LeakyReLU} en el módulo \lstinline{keras.layers.advanced_activations}.
\item \lstinline{Dropout}: Consiste en establecer al azar una fracción de unidades de entrada a 0 en cada actualización durante la fase de entrenamiento. Esto ayuda a evitar el sobreajuste.
\lstinline{keras.layers.core.Dropout(p)}
\item \lstinline{Reshape}: Transforma la salida en una forma concreta.
\end{itemize}
Algunos ejemplos de uso prodrían ser:
\begin{itemize}[noitemsep]
\item \lstinline{model.add(Dense(32, input_dim=16))} : modelo que contaría con una array de entrada de tamaño 16, y de salida de 32.
\item \lstinline{model.add(Activation('tanh'))} : aplica la función de activación tangencial.
\item \lstinline{model.add(Dropout(0.2))} : establecería entradas a 0 en el 20\% de los casos.
\item \lstinline{model.add(Reshape((3, 4), input_shape=(12,)))} : define una capa de salida de 3x4 para nuestro modelo.
\end{itemize}

\subsection{Capas convolucionales}
Keras presenta diversas funciones para desarrollar capas convolucionales según las dimensiones de su entrada. La más destacable, a modo de ejemplo, es:\\
\lstinline{Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)}\\\\
Un ejemplo de uso donde se define una capa convolucional de dos dimensiones, con 32 mapeadores de tamaño 5x5 y una funcion de activación rectificadora, sería:\\
\lstinline{model.add(Convolution2D(32, 5, 5, border_mode='valid', activation='relu'))} : \\\\
Además de la versión en 2D, Keras nos ofrece funciones para entradas de 1 dimensión o 3 dimensiones. Aparte, tenemos funcionalidades para desarrollar capas convolucionales dilatadas (\lstinline{AtrousConvolution2D}), hacer deconvolución (\lstinline{Deconvolution2D}), etc.
\subsection{Capas pooling}
Al igual que ocurre con las capas convolucionales, Keras ofrece un buen numero de funciones para desarrollar capas pooling en redes convolucionales, dependiendo de las dimensiones de la entrada. Como en el caso anterior, una función representativa sería:\\
\lstinline{MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')}\\\\
Si en vez de utilizar la función máximo se requiere que nuestra capa utilice otro tipo de algoritmos, Keras también nos ofrece funciones como: \lstinline{AveragePooling2D}, \lstinline{GlobalMaxPooling2D}, \lstinline{GlobalAveragePooling2D}... Además de sus variantes para 1D y 3D.
\subsection{Capas recurrentes}
Keras presenta 3 tipos de capas recurrentes:
\begin{itemize}[noitemsep]
\item \lstinline{SimpleRNN} : Red recurrente estándar totalmente conectada cuya salida realimenta la entrada.
\item \lstinline{LSTM} : Redes Long-Short Term Memory. Este tipo de red ha sido descrito en el apartado \ref{redes_neuronales_recurrentes}, y será puesto en ejemplo en el apartado \ref{ej_lstm}
\item \lstinline{GRU} : Gated Recurrent Unit. Red neuronal recurrente muy similar a las redes LSTM, pero que no dispone de una unidad de memoria interna y utiliza dos puertas en lugar de tres.
\end{itemize}

\lsection{Inicializadores}
Los inicializadores definen la manera de inicializar los pesos de las capas de nuestro modelo. El argumento usado en para definir este inicializador en las capas es \textbf{init}. Las opciones más comunes son:
\begin{itemize}[noitemsep]
\item uniform
\item normal
\item identity
\item zero
\item glorot\_normal
\item glorot\_uniform
\end{itemize}

\lsection{Activaciones}
El modo de activación puede ser implementado mediante una capa \lstinline{Activation} (\lstinline{model.add(Dense(64)); model.add(Activation('tanh')))}, o mediante un parámetro que soportan todas las capas (\lstinline{model.add(Dense(64, activation='tanh'))}).\\
Las funciones más comunes, descritas previamente en el punto \ref{def_funcion_activacion}, son :
\begin{itemize}[noitemsep]
\item linear: función lineal.
\item sigmoid: función sigmoide / logística.
\item tanh: función tangente hiperbólica.
\item relu: función rectificadora.
\item softplus: aproximación suavizada de la función rectificadora.
\end{itemize}
\lsection{Funciones de pérdida}
La función de perdida es uno de los dos parámetros necesarios para compilar un modelo. Algunas de las funciones más conocidas que nos proporciona Keras son:
\begin{itemize}[noitemsep]
\item \lstinline{mean_squared_error(y_true, y_pred)} : Calcula el error cuadrático medio.
\item \lstinline{mean_absolute_error / mae} : Calcula el error medio absoluto.
\item \lstinline{binary_crossentropy}: Calcula la entropía cruzada en problemas de clasificación binaria. También conocida como perdida logarítmica.
\end{itemize}
\subsection{Regularizadores}
Los regularizadores permiten aplicar penalizaciones en las capas durante la optimización. Estas penalizaciones se incorporan a la función de perdida que la red optimiza.\\
Ejemplo de uso:
\lstinline{model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))}

\lsection{Optimizadores}
El optimizador es otro de los dos valores necesarios para compilar un modelo (junto con la función de pérdida). Los principales optimizadores disponibles son:
\begin{itemize}[noitemsep]
\item \lstinline{SGD} : aproximación estocástica del metodo de gradiente descendiente descrito en los puntos \ref{backpropagation} y \ref{regla_delta}.
\item \lstinline{Adagrad} : algoritmo basado en gradiente que adapta el ratio de aprendizaje a los parámetros, realizando grandes actualizaciones cuando éstos son infrecuentes, y pequeñas actualizaciones cuando son frecuentes.
\item \lstinline{Adadelta} : extensión del algoritmo Adagrad
\item \lstinline{Adam} : algoritmo basado en el gradiente de primer orden de funciones estocásticas objetivas. Este método es sencillo de implementar, es eficiente computacionalmente hablando, necesita poca capacidad de memoria, y es muy bueno en problemas con una gran cantidad de datos y/o parámetros. Es utilizado por defecto en los modelos de Keras.
\end{itemize}

\lsection{Callbacks}
Los callbacks son una serie de funciones que pueden ser aplicadas en ciertos momentos del proceso de entrenamiento. Estos callbacks pueden ser utilizados para echar un vistazo a los estados internos así como a las estadísticas del modelo que estamos entrenando. Para utilizarlos, basta con pasar como argumento a la función \lstinline{fit(...)} una lista de métodos, que serán llamados en cada fase del entrenamiento.\\
Las principales funciones son:
\begin{itemize}
\item \lstinline{BaseLogger()}: Se aplica en todo modelo de Keras, y acumula la media de las métricas por época que se han definido para ser monitorizadas.
\item \lstinline{Callback()}: Clase base abstracta, utilizada para crear nuevos callbacks. Como parámetros, recibe los propios para configurar el entrenamiento (numero de ciclos, verbosidad...), así como la instancia del modelo que queremos entrenar.
\item \lstinline{ProgbarLogger()}: Callback que muestra métricas en la salida estándar (stdout).
\item \lstinline{History()}: Callback aplicado en cada modelo Keras, que almacena eventos en un objeto de tipo \lstinline{History}.
\item \lstinline{ModelCheckpoint(...)}: Almacena el modelo después de cada época. Como argumentos recibe el fichero donde dejar la información, la cantidad de de elementos a monitorizar, el modo de verbosidad, un booleano para decidir si sobreescribir o no el mejor modelo, el modo, y otro booleano para guardar solo pesos o información de todo el modelo.
\end{itemize}

\lsection{Datasets}
Keras cuenta con una serie de dataset ya predefinidos y listos para realizar nuestras pruebas:
\begin{itemize}
\item \textbf{CIFAR10}: dataset que cuenta con 50.000 imágenes a color de entrenamiento, a clasificar en 10 categorías, y 10.000 imágenes para testear los resultados. También existe una versión con 100 categorías (\textbf{CIFAR100}).
\item \textbf{Clasificación de sentimientos en reviews de IMDB}: Este dataset cuenta con 25.000 películas etiquetadas según el sentimiento (bueno/malo). Estas reviews han sido preprocesadas, y las palabras han sido indexadas según su frecuencia.
\item \textbf{MNIST}: dataset que contiene 60.000 imágenes en blanco y negro de 10 tipos de dígitos, y 10.000 imágenes para testear los modelos.
\end{itemize}

\lsection{Preprocesado}
Keras cuenta con una serie de funciones para procesar tanto modelos secuenciales, como texto, como imágenes. Las más importantes son:
\begin{itemize}
\item Modelos secuenciales: \lstinline{pad_sequences(...)} transforma una lista en un array 2D.
\item Texto: \lstinline{text_to_word_sequence(...)} separa una frase en palabras, \lstinline{one_hot(...)} codifica un texto en una lista de palabras indexadas en un vocabulario de tamaño n...
\item Imágenes: \lstinline{ImageDataGenerator(...)} define la configuración para la preparación y el aumento de una imágen, a la hora de ser procesada.
\end{itemize}

\lsection{Visualización}
El módulo \lstinline{keras.utils.visualize_util} proporciona funciones útiles para pintar gráficamente un modelo (acompañado del uso de \lstinline{graphviz}). La siguiente función mostraría una gráfica del modelo y lo guardaría en un fichero .png.\\
\lstinline{plot(model, to_file='model.png')}
\lsection{Wrapper para Scikit-Learn API}
Keras presenta compatibilidad con Scikit-Learn API. Esta conocida librería open source proporciona herramientas para la mineria y analisis de datos. Trabaja junto con las librerias SciPy y NumPy de Python, y permite resolver problemas de regresión, clasificación y clustering.\\
Se pueden usar modelos Keras secuenciales como parte del ciclo de trabajo de Scikit-Learn mediante sus wrappers. Existen dos interfaces disponibles:
\begin{itemize}[noitemsep]
\item \lstinline{KerasClassifier(build_fn=None, **sk_params)} : Implementa una interfaz de clasificación.
\item \lstinline{KerasRegressor(build_fn=None, **sk_params)} : Implementa una interfaz de regresión.
\end{itemize}
El primer argumento es la instancia de la clase o la función a llamar, y el segundo los parámetros del modelo y de ajuste.
