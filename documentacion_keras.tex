\chapter{Keras}
\section{Introducción}
Keras es una librería de redes neuronales escrita en Python y capaz de correr tanto en Tensorflow como en Theano. Sus principales características son:
\begin{itemize}[noitemsep]
\item Fácil y rápido prototipado gracias a su modularidad, minimalismo y extensibilidad.
\item Soporta tanto redes neuronales convolucionales como recurrentes (así como la combinación de ambas)
\item Soporta esquemas de conectividad arbitrarios (incluyendo entrenamiento multi-entrada y multi-salida)
\item Corre en CPU y GPU
\item Es compatible con Python 2.7-3.5
 \end{itemize}

 \section{Instalación}
 Para la instalacion de Keras, son necesarias las siguientes dependencias:
 \begin{itemize}[noitemsep]
\item numpy y scipy
\item pyyaml
\item HDF5 y h5py (Opcional, pero requírido si se usan funciones para cargar/guardar modelos)
\item cuDNN (Opcional pero recomendado si se usan CNNs)
\item Tensorflow o Theano
 \end{itemize}
Existen 2 modos de instalacion:
 \begin{itemize}[noitemsep]
 \item Desde PyPI: \lstinline{sudo pip install keras}
 \item Desde repositorio Keras (https://github.com/fchollet/keras/tree/master/keras): \lstinline{sudo python setup.py install}
 \end{itemize}
 Por defecto, Keras usa Tensorflow como motor backend. Sin embargo, esta opción puede ser configurada. La primera vez se ejecuta Keras, se crea un fichero de configuración en "/.keras/keras.json", que puede ser modificado.
\begin{lstlisting}
    "image_dim_ordering": "tf",
    "epsilon": 1e-07,
    "floatx": "float32",
    "backend": "tensorflow"
\end{lstlisting}

\section{Modelos}
Hay 2 tipos de modelos disponibles para su implementacion en Keras: el modelo secuencial, y la clase Model (más general). Ambos modelos presentan varias funciones en común, como \lstinline{summary()}, \lstinline{get_config()} o \lstinline{to_json()} que devuelven información basica sobre el modelo, o \lstinline{get\_weights()} y \lstinline{set\_weights()}, las funciones getter y setter para los pesos del modelo.
\subsection{Modelo secuencial}
Las funciones básicas del modelo secuencial son:
\begin{itemize}
\item \lstinline{compile(self, optimizer, loss, metrics=[], sample_weight_mode=None)} : Configura el proceso de aprendizaje.
\item \lstinline{fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)} : Entrena el modelo para un número fijado de ciclos.
\item \lstinline{evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)} : Calcula la función de perdida dados unos datos de entrada, lote por lote.
\item \lstinline{predict(self, x, batch_size=32, verbose=0)} : Genera predicciones para unos valores de entrada.
\end{itemize}
\subsection{Clase Model de la API}
Con la clase Model, dadas una entrada y una salida, podemos inicializar nuestro modelo de la siguiente forma:
\begin{lstlisting}[language=Python]
a = Input(shape=(32,))
b = Dense(32)(a)
model = Model(input=a, output=b)
\end{lstlisting}
En caso de que tengamos múltiples conjuntos de entradas y de salidas, también podríamos inicializarlo de la siguiente manera:
\begin{lstlisting}[language=Python]
model = Model(input=[a1, a2], output=[b1, b3, b3])
\end{lstlisting}
\section{Capas}
\subsection{Funciones básicas de las capas}
A la hora de diseñar las capas de nuestra red neuronal, tenemos una serie de funciones disponibles:
\begin{itemize}
\item Dense: Para capas regulares totalmente conectadas.\\
\lstinline{keras.layers.core.Dense(output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)}\\
Ejemplo de uso: \lstinline{model.add(Dense(32, input_dim=16))} sería un modelo que contaría con una array de entrada de tamaño 16, y de salida de 32.
\item Activation: Se aplica una función de activación a una salida. Las principales funciones que nos ofrece Keras son:
\begin{itemize}
\item softmax: generalización de la función logística.
\item softplus
\item softsign
\item relu: función rectificadora
\item tanh: función tangente hiperbólica
\item sigmoid: función sigmoide
\item hard\_sigmoid
\item linear: función lineal
\end{itemize}
Podemos encontrar funciones de activación mas avanzadas como PReLu o LeakyReLU en el modulo \lstinline{keras.layers.advanced_activations}.\\
Ejemplo de uso: \lstinline{model.add(Activation('tanh'))}
\item Dropout: Proporciona una manera simple de prevenir que se produzca sobreajuste.\\
\lstinline{keras.layers.core.Dropout(p)}
\item Reshape: Transforma la salida en una forma concreta.\\
Ejemplo de uso: \lstinline{model.add(Reshape((3, 4), input_shape=(12,)))} definiría una capa de salida de 3x4 para nuestro modelo.
\item Permute: Transforma la dimensión de la entrada en un patrón definido.\\
Ejemplo de uso: \lstinline{model.add(Permute((2, 1), input_shape=(10, 64)))}
\end{itemize}

\subsection{Capas convolucionales}
Keras presenta diversas funciones para desarrollar capas convolucionales según las dimensiones de su entrada. La mas destacable, a modo de ejemplo, podría ser:\\
\lstinline{Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)}\\\\
Además de la versión en 2D, Keras nos ofrece funciones para entradas de 1 dimensión o 3 dimensiones. A parte, tenemos funcionalidades para desarrollar capas convolucionales dilatadas (AtrousConvolution2D), hacer deconvolución (Deconvolution2D), cropping (Cropping2D)...
\subsection{Capas pooling}
Al igual que ocurre con las capas convolucionales, Keras ofrece un buen numero de funciones para desarrollar capas pooling en redes convolucionales, dependiendo de las dimensiones de la entrada. Como en el caso anterior, una función representativa sería:\\
\lstinline{MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')}\\\\
Si en vez de utilizar la función máximo se requiere que nuestra capa utilice otro tipo de algoritmos, Keras también nos ofrece funciones como: AveragePooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D... Además de sus variantes para 1D y 3D.
\subsection{Capas recurrentes}
Keras presenta 3 tipos de capas recurrentes:
\begin{itemize}[noitemsep]
\item SimpleRNN : Red recurrente totalmente conectada cuya salida realimenta la entrada.
\item LSTM : Long-Short Term Memory unit
\item GRU : Gated Recurrent Unit
\end{itemize}

\section{Preprocesado}
Keras cuenta con una serie de funciones para procesar tanto modelos secuenciales, como texto, como imágenes. Las más importantes son:
\begin{itemize}
\item Modelos secuenciales: \lstinline{pad_sequences(...)} transforma una lista en un array 2D.
\item Texto: \lstinline{text_to_word_sequence(...)} separa una frase en palabras, \lstinline{one_hot(...)} codifica un texto en una lista de palabras indexadas en un vocabulario de tamaño n...
\item Imágenes: \lstinline{ImageDataGenerator(...)} genera ciclos con los datos de la imagen con aumento en tiempo real.
\end{itemize}
\section{Funciones de pérdida}
La función de perdida es uno de los dos parámetros necesarios para compilar un modelo. Algunas de las funciones mas conocidas que nos proporciona Keras son:
\begin{itemize}[noitemsep]
\item \lstinline{mean_squared_error(y_true, y_pred)} : Calcula el error cuadrático medio.
\item \lstinline{mean_absolute_error / mae} : Calcula el error medio absoluto.
\item \lstinline{binary_crossentropy}: Calcula la entropía cruzada en problemas de clasificación binaria. También conocida como perdida logarítmica.
\end{itemize}
\section{Optimizadores}
El optimizador es otro de los dos valores necesarios para compilar un modelo (junto con la función de pérdida). En los ejemplos que se describirán mas adelante, se usará el optimizador Adam.
\begin{itemize}
\item \lstinline{Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)}
\end{itemize}
Adam es un algoritmo para la optimización basado en el gradiente de primer orden de funciones estocásticas objetivas. Está basado en estimaciones adaptativas de momentos de orden inferior. Este método es sencillo de implementar, es eficiente computacionalmente hablando, necesita poca capacidad de memoria, y es muy bueno en problemas con una gran cantidad de datos y/o parámetros.
\section{Activadores}
El modo de activación puede ser implementado mediante una capa Activation (\lstinline{model.add(Dense(64)); model.add(Activation('tanh')))}, o mediante un parámetro que soportan todas las capas (\lstinline{model.add(Dense(64, activation='tanh'))}).\\
Las funciones más comunes son:
\begin{itemize}[noitemsep]
\item softmax
\item softsign
\item tanh
\item sigmoid
\item linear
\end{itemize}
\section{Callbacks}
Los callbacks son una serie de funciones que pueden ser aplicadas en ciertos momentos del proceso de entrenamiento. Estos callbacks pueden ser utilizados para echar un vistazo a los estados internos así como a las estadísticas del modelo que estamos entrenando. Para utilizarlos, basta con pasar como argumento a la función \lstinline{fit(...)} una lista de métodos, que serán llamados en cada fase del entrenamiento.\\
Las principales funciones son:
\begin{itemize}
\item \lstinline{BaseLogger()}: Se aplica en todo modelo de Keras, y acumula la media de las métricas por ciclo.
\item \lstinline{Callback()}: Clase base abstracta, utilizada para crear nuevos callbacks. Como parámetros, recibe los propios para configurar el entrenamiento (numero de ciclos, verbosidad...), así como la instancia del modelo que queremos entrenar.
\item \lstinline{ProgbarLogger()}: Callback que pinta métricas en la salida estándar (stdout).
\item \lstinline{History()}: Callback aplicado en cada modelo Keras, que almacena eventos en un objeto de tipo History.
\item \lstinline{ModelCheckpoint(...)}: Almacena el modelo después de cada ciclo. Como argumentos recibe el fichero donde dejar la información, la cantidad de de elementos a monitorizar, el modo de verbosidad, un booleano para decidir si sobreescribir o no el mejor modelo, el modo, y otro booleano para guardar solo pesos o información de todo el modelo.
\end{itemize}
\section{Datasets}
Keras cuenta con una serie de dataset ya predefinidos y listos para realizar nuestras pruebas:
\begin{itemize}
\item \textbf{CIFAR10}: dataset que cuenta con 50.000 imágenes a color de entrenamiento, a clasificar en 10 categorías, y 10.000 imágenes para testear los resultados. También existe una versión con 100 categorías (\textbf{CIFAR100}).
\item \textbf{Clasificación de sentimientos en reviews de IMDB}: Este dataset cuenta con 25.000 películas etiquetadas según el sentimiento (bueno/malo). Estas reviews han sido preprocesadas, y las palabras han sido indexadas según su frecuencia.
\item \textbf{MNIST}: dataset que contiene 60.000 imágenes en blanco y negro de 10 tipos de dígitos, y 10.000 imágenes para testear los modelos.
\end{itemize}
\section{Inicializadores}
Los inicializadores definen la manera de inicializar los pesos de las capas de nuestro modelo. El argumento usado en para definir este inicializador en las capas es \textbf{init}. Las opciones mas comunes son:
\begin{itemize}[noitemsep]
\item uniform
\item normal
\item identity
\item zero
\end{itemize}
\section{Regularizadores}
Los regularizadores permiten aplicar penalizaciones en las capas durante la optimización. Estas penalizaciones se incorporan a la función de perdida que la red optimiza.\\
Ejemplo de uso:
\lstinline{model.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))}
\section{Visualización}
El módulo \textbf{keras.utils.visualize\_util} proporciona funciones útiles para pintar gráficamente un modelo (acompañado del uso de graphviz). La siguiente función pintaría una gráfica del modelo y lo guardaría en un fichero .png.\\
\lstinline{plot(model, to_file='model.png')}
\section{Wrapper para Scikit-Learn API}
Keras presenta compatibilidad con Scikit-Learn API. Se pueden usar modelos Keras secuenciales como parte del ciclo de trabajo de Scikit-Learn mediante sus wrappers. Hay 2 disponibles:
\begin{itemize}[noitemsep]
\item \lstinline{KerasClassifier(build_fn=None, **sk_params)} : Implementa una interfaz de clasificación.
\item \lstinline{KerasRegressor(build_fn=None, **sk_params)} : Implementa una interfaz de regresión.
\end{itemize}
El primer argumento es la instancia de la clase o la función a llamar, y el segundo los parámetros del modelo y de ajuste.
